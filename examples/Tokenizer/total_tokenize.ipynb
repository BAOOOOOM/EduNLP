{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 令牌化\r\n",
    "\r\n",
    "## 概述\r\n",
    "\r\n",
    "此模块可以定制化的将文本切分为令牌（token）序列，在此，将展示三种方式来进行令牌化：调用tokenize函数、调用sif4sci函数、调用封装好的tokenizer。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## tokenize\r\n",
    "\r\n",
    "### 概述\r\n",
    "\r\n",
    "此模块为令牌化的基础实现模块，仅需将分词后的item列表传入此函数中即可得到相应的令牌化结果。\r\n",
    "\r\n",
    "根据构成题目的元素类型，解析功能分为 **“文本解析”** 和 **“公式解析”** 两部分。\r\n",
    "\r\n",
    "#### 文本解析\r\n",
    "\r\n",
    "根据题目文本切分粒度的大小，文本解析又分为 **“句解析”** 和 **“词解析”**。\r\n",
    "\r\n",
    "（1） 句解析（sentence-tokenization）：将较长的文档切分成若干句子的过程称为“分句”。每个句子为一个“令牌”（token）。（待实现）    \r\n",
    "  \r\n",
    "\r\n",
    "（2） 词解析（text-tokenization）：一个句子（不含公式）是由若干“词”按顺序构成的，将一个句子切分为若干词的过程称为“词解析”。根据词的粒度大小，又可细分为“词组解析”和\"单字解析\"。\r\n",
    "- 词组解析 (word-tokenization)：每一个词组为一个“令牌”（token）。\r\n",
    "- 单字解析 (char-tokenization)：单个字符即为一个“令牌”（token）。\r\n",
    "\r\n",
    "#### 公式解析\r\n",
    "\r\n",
    "公式解析（formula-tokenization）：理科类文本中常常含有公式。将一个符合 latex 语法的公式切分为标记字符列表的过程称为“公式解析”。每个标记字符为一个“令牌”（token）。  \r\n",
    "  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 文本解析"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 句解析\r\n",
    "\r\n",
    "待实现..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 词解析\r\n",
    "\r\n",
    "词解析分为两个主要步骤：  \r\n",
    "\r\n",
    "（1） 分词：  \r\n",
    "- 词组解析：使用分词工具切分并提取题目文本中的词。  \r\n",
    "    本项目目前支持的分词工具有：`jieba`   \r\n",
    "- 单字解析：按字符划分。\r\n",
    "    \r\n",
    "      \r\n",
    "（2） 筛选：过滤指定的停用词。   \r\n",
    "- 本项目默认使用的停用词表：[stopwords](https://github.com/bigdata-ustc/EduNLP/blob/master/EduNLP/meta_data/sif_stopwords.txt)  \r\n",
    "- 你也可以使用自己的停用词表，具体使用方法见下面的示例。\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 导入模块\r\n",
    "from EduNLP.SIF.tokenization.text import tokenize "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 输入\r\n",
    "text = \"三角函数是基本初等函数之一\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 词组解析\r\n",
    "\r\n",
    "分词粒度参数选择 word： `granularity = \"word\"` "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 输出：默认使用 EduNLP 项目提供的停用词表\r\n",
    "tokenize(text, granularity=\"word\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "['三角函数', '初等', '函数']"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 单字解析\r\n",
    "\r\n",
    "分词粒度参数选择 word： `granularity = \"char\"` "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 输出：默认使用 EduNLP 项目提供的停用词表\r\n",
    "tokenize(text, granularity=\"char\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "['三', '角', '函', '数', '基', '初', '函', '数']"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 停用词表"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 获取自己的停用词表\r\n",
    "spath = \"test_stopwords.txt\"\r\n",
    "from EduNLP.SIF.tokenization.text.stopwords import get_stopwords\r\n",
    "stopwords = get_stopwords(spath)\r\n",
    "stopwords"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "{'一旦', '一时', '一来', '一样', '一次', '一片', '一番', '一直', '一致'}"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 输出：传入停用词表（stopwords）\r\n",
    "tokenize(text,granularity=\"word\",stopwords=stopwords)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "['三角函数', '是', '基本', '初等', '函数', '之一']"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 公式解析\r\n",
    "切分出 latex 公式的每个标记符号。针对本模块更加详细的解释参见 [formula](../formula/formula.ipynb)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 导入模块\r\n",
    "from EduNLP.SIF.tokenization.formula import tokenize"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 输入"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "formula = \"\\\\frac{\\\\pi}{x + y} + 1 = x\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 输出"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "（1）如果您想按 latex 语法标记拆分公式的各个部分，并得到顺序序列结果，输出方法可以选择：`linear`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenize(formula, method=\"linear\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "['\\\\frac', '{', '\\\\pi', '}', '{', 'x', '+', 'y', '}', '+', '1', '=', 'x']"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "（2） 如果您想得到公式解析出的语法分析树序列，输出方法可以选择：`ast`\n",
    "> 抽象语法分析树，简称语法树（Syntax tree），是源代码语法结构的一种抽象表示。它以树状的形式表现编程语言的语法结构，树上的每个节点都表示源代码中的一种结构。  \n",
    "> 因此，ast 可以看做是公式的语法结构表征。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenize(formula, method=\"ast\", return_type=\"list\", ord2token=False)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "['\\\\pi', '{ }', 'x', '+', 'y', '{ }', '\\\\frac', '+', '1', '=', 'x']"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "（3）如果您只是关心公式的结构和类型，并不关心变量具体是什么，比如二元二次方程 `x^2 + y = 1` ，它从公式结构和类型上来说，和 `w^2 + z = 1` 没有区别。  \n",
    "此时，您可以设置如下参数：`ord2token = True`，将公式变量名转换成 token"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 输出形式选择抽象语法分析树（ast）且将公式变量名转换成 token\r\n",
    "tokenize(formula, method=\"ast\", return_type=\"list\", ord2token=True)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "['mathord',\n",
       " '{ }',\n",
       " 'mathord',\n",
       " '+',\n",
       " 'mathord',\n",
       " '{ }',\n",
       " '\\\\frac',\n",
       " '+',\n",
       " 'textord',\n",
       " '=',\n",
       " 'mathord']"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "（4） 如果您除了 （3） 中提供的功能之外，还需要区分不同的变量。此时可以另外设置参数：`var_numbering=True`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 输出形式选择抽象语法分析树（ast）且将公式变量名转换成带编号的 token\r\n",
    "tokenize(formula, method=\"ast\", return_type=\"list\", ord2token=True, var_numbering=True)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "['mathord_con',\n",
       " '{ }',\n",
       " 'mathord_0',\n",
       " '+',\n",
       " 'mathord_1',\n",
       " '{ }',\n",
       " '\\\\frac',\n",
       " '+',\n",
       " 'textord',\n",
       " '=',\n",
       " 'mathord_0']"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 综合解析\r\n",
    "\r\n",
    "综合解析，即综合以上两种解析方式（标记解析 + 公式解析），提供对题目文本的全解析。另外，如遇到特殊符号将转换成常量，例如：\r\n",
    "```python\r\n",
    "FIGURE_SYMBOL = \"[FIGURE]\" # $\\SIFChoice$\r\n",
    "QUES_MARK_SYMBOL = \"[MARK]\" # $\\FigureID{1}$\r\n",
    "```\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 导入模块\r\n",
    "from EduNLP.Tokenizer import get_tokenizer\r\n",
    "\r\n",
    "# 输入\r\n",
    "item = {\r\n",
    "    \"如图来自古希腊数学家希波克拉底所研究的几何图形．此图由三个半圆构成，三个半圆的直径分别为直角三角形$ABC$的斜边$BC$, 直角边$AB$, $AC$.$\\bigtriangleup ABC$的三边所围成的区域记为$I$,黑色部分记为$II$, 其余部分记为$III$.在整个图形中随机取一点，此点取自$I,II,III$的概率分别记为$p_1,p_2,p_3$,则$\\SIFChoice$$\\FigureID{1}$\"\r\n",
    "}\r\n",
    "\r\n",
    "# 输出\r\n",
    "tokenizer = get_tokenizer(\"text\")\r\n",
    "tokens = tokenizer(item)\r\n",
    "next(tokens) "
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "['如图',\n",
       " '古希腊',\n",
       " '数学家',\n",
       " '希波',\n",
       " '克拉底',\n",
       " '研究',\n",
       " '几何图形',\n",
       " '此图',\n",
       " '三个',\n",
       " '半圆',\n",
       " '三个',\n",
       " '半圆',\n",
       " '直径',\n",
       " '直角三角形',\n",
       " 'ABC',\n",
       " '斜边',\n",
       " 'BC',\n",
       " '直角',\n",
       " 'AB',\n",
       " 'AC',\n",
       " '\\x08',\n",
       " 'igtriangleupABC',\n",
       " '三边',\n",
       " '围成',\n",
       " '区域',\n",
       " '记',\n",
       " 'I',\n",
       " '黑色',\n",
       " '记',\n",
       " 'II',\n",
       " '其余部分',\n",
       " '记',\n",
       " 'III',\n",
       " '图形',\n",
       " '中',\n",
       " '随机',\n",
       " '取',\n",
       " '一点',\n",
       " '此点',\n",
       " '取自',\n",
       " 'I',\n",
       " ',',\n",
       " 'II',\n",
       " ',',\n",
       " 'III',\n",
       " '概率',\n",
       " '记',\n",
       " 'p',\n",
       " '_',\n",
       " '1',\n",
       " ',',\n",
       " 'p',\n",
       " '_',\n",
       " '2',\n",
       " ',',\n",
       " 'p',\n",
       " '_',\n",
       " '3',\n",
       " '[MARK]',\n",
       " '[FIGURE]']"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## tokenizer\r\n",
    "\r\n",
    "### 概述\r\n",
    "\r\n",
    "tokenizer为一个封装好的令牌化容器，让用户可以渐变的将item转换为令牌化列表，下面将介绍两种令牌化容器。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from EduNLP.Tokenizer import PureTextTokenizer, TextTokenizer, get_tokenizer"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\MySoftwares\\Anaconda\\envs\\data\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TextTokenizer and PureTextTokenizer\r\n",
    "\r\n",
    "- ‘text’ Tokenizer ignores and skips the FormulaFigures and tokenize latex Formulas as Text\r\n",
    "- ‘pure_text’ Tokenizer symbolizes the FormulaFigures as [FUMULA] and tokenize latex Formulas as Text"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TextTokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "items = [{\r\n",
    "        \"stem\": \"已知集合$A=\\\\left\\\\{x \\\\mid x^{2}-3 x-4<0\\\\right\\\\}, \\\\quad B=\\\\{-4,1,3,5\\\\}, \\\\quad$ 则 $A \\\\cap B=$\",\r\n",
    "        \"options\": [\"1\", \"2\"]\r\n",
    "        }]\r\n",
    "tokenizer = get_tokenizer(\"text\") # tokenizer = TextTokenizer()\r\n",
    "tokens = tokenizer(items, key=lambda x: x[\"stem\"])\r\n",
    "print(next(tokens))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['已知', '集合', 'A', '=', '\\\\left', '\\\\{', 'x', '\\\\mid', 'x', '^', '{', '2', '}', '-', '3', 'x', '-', '4', '<', '0', '\\\\right', '\\\\}', ',', '\\\\quad', 'B', '=', '\\\\{', '-', '4', ',', '1', ',', '3', ',', '5', '\\\\}', ',', '\\\\quad', 'A', '\\\\cap', 'B', '=']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "items = [\"有公式$\\\\FormFigureID{wrong1?}$，如图$\\\\FigureID{088f15ea-xxx}$,若$x,y$满足约束条件公式$\\\\FormFigureBase64{wrong2?}$,$\\\\SIFSep$，则$z=x+7 y$的最大值为$\\\\SIFBlank$\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "tokenizer = get_tokenizer(\"text\") # tokenizer = TextTokenizer()\r\n",
    "tokens = [t for t in tokenizer(items)]\r\n",
    "tokens"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "[['公式',\n",
       "  '[FORMULA]',\n",
       "  '如图',\n",
       "  '[FIGURE]',\n",
       "  'x',\n",
       "  ',',\n",
       "  'y',\n",
       "  '约束条件',\n",
       "  '公式',\n",
       "  '[FORMULA]',\n",
       "  '[SEP]',\n",
       "  'z',\n",
       "  '=',\n",
       "  'x',\n",
       "  '+',\n",
       "  '7',\n",
       "  'y',\n",
       "  '最大值',\n",
       "  '[MARK]']]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### PureTextTokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer = get_tokenizer(\"pure_text\") # tokenizer = PureTextTokenizer()\r\n",
    "tokens = [t for t in tokenizer(items)]\r\n",
    "tokens"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "[['公式',\n",
       "  '如图',\n",
       "  '[FIGURE]',\n",
       "  'x',\n",
       "  ',',\n",
       "  'y',\n",
       "  '约束条件',\n",
       "  '公式',\n",
       "  '[SEP]',\n",
       "  'z',\n",
       "  '=',\n",
       "  'x',\n",
       "  '+',\n",
       "  '7',\n",
       "  'y',\n",
       "  '最大值',\n",
       "  '[MARK]']]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}